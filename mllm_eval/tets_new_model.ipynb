{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pip install transformers>=4.35.2\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from utils import merge_images, load_image\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "class XComposer2d5():\n",
    "    support_multi_image = True\n",
    "    def __init__(self, model_path:str=\"internlm/internlm-xcomposer2d5-7b\", eval_mode=None) -> None:\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda().eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model.tokenizer = self.tokenizer\n",
    "        self.eval_mode = eval_mode\n",
    "        \n",
    "    def __call__(self, inputs: dict) -> str:\n",
    "        if self.eval_mode.startswith('single_choice'):\n",
    "            try:\n",
    "                generated_text = self.get_single_choice_anwser(inputs)\n",
    "            except:\n",
    "                return 'ERROR!!!'\n",
    "        elif self.eval_mode.startswith('flow_insert'):\n",
    "            try:\n",
    "                generated_text = self.get_flow_insert_answer(inputs)\n",
    "            except:\n",
    "                return 'ERROR!!!'\n",
    "        return generated_text\n",
    "\n",
    "    def get_single_choice_anwser(self, inputs: dict) -> str:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs : {\n",
    "                'above_content':\n",
    "                'below_content: \n",
    "                'images': [\n",
    "                    \n",
    "                ]\n",
    "                'temple_img': \n",
    "                'temple_txt': \n",
    "            }\n",
    "        \"\"\"\n",
    "        temple_txt = inputs['temple_txt']\n",
    "        temple_img = inputs['temple_img']\n",
    "        if self.support_multi_image:\n",
    "            query = temple_txt + inputs['above_content'] + '\\n' + inputs['below_content'] \\\n",
    "                + temple_img + 'A: <ImageHere>; B: <ImageHere>; C: <ImageHere>'\n",
    "            image = inputs['images']\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                generated_text, _ = self.model.chat(self.tokenizer, query, image, do_sample=False, num_beams=1, use_meta=True)\n",
    "\n",
    "            return generated_text\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        \n",
    "    def get_flow_insert_answer(self, inputs: dict) -> str:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs : {\n",
    "                'paragraphs': \n",
    "                'image': \n",
    "                'temple_img': \n",
    "                'temple_txt': \n",
    "            }\n",
    "        \"\"\"\n",
    "        temple_txt = inputs['temple_txt']\n",
    "        temple_img = inputs['temple_img']\n",
    "        if self.support_multi_image:\n",
    "            query = temple_txt + inputs['paragraphs'] + '\\n' \\\n",
    "                + temple_img + '<ImageHere>'\n",
    "            image = inputs['images']\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                generated_text, _ = self.model.chat(self.tokenizer, query, image, do_sample=False, num_beams=1, use_meta=True)\n",
    "\n",
    "            return generated_text\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bunny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
